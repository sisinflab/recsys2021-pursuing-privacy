{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we will learn to implement a simple differentially private matrix factorization from scratch, with three different strategies:\n",
    "* input perturbation with Laplacian mechanism\n",
    "* gradient perturbation with Laplacian mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (4.62.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (from pandas) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (from pandas) (1.19.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/antonioferrara/Documents/virtual_environments/RecSysPrivacyTutorial/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import create_maps, splitting\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load Data\n",
    "\n",
    "First, we download the latest version of the Movielens Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Movielens Small from : https://files.grouplens.org/datasets/movielens/ml-latest-small.zip ...\n",
      "Extracting ratings...\n",
      "Printing ratings to data/movielens/ ...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "print(f\"Getting Movielens Small from : {url} ...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "ml_ratings = []\n",
    "\n",
    "print(\"Extracting ratings...\")\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "    for line in zip_ref.open(\"ml-latest-small/ratings.csv\"):\n",
    "        ml_ratings.append(str(line, \"utf-8\"))\n",
    "\n",
    "print(\"Printing ratings to data/movielens/ ...\")\n",
    "os.makedirs(\"data/movielens\", exist_ok=True)\n",
    "with open(\"data/movielens/dataset.csv\", \"w\") as f:\n",
    "    f.writelines(ml_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing splitting...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataframe_ml_small = pd.read_csv('data/movielens/dataset.csv')\n",
    "\n",
    "train_set, test_set = splitting(dataframe_ml_small)\n",
    "train_set = train_set.loc[:, ['userId', 'movieId', 'rating']]\n",
    "test_set = test_set.loc[:, ['userId', 'movieId', 'rating']]\n",
    "maps = create_maps(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Create MF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MF:\n",
    "    def __init__(self, dataset, maps, n_factors, relevance=3.5, i_avg=None, u_avg=None):\n",
    "        \"\"\"\n",
    "        :param dataset: interaction dataset should be a Pandas dataframe with three columns for user, item, and rating\n",
    "        :param n_factors:\n",
    "        \"\"\"\n",
    "        print(\"Building model...\")\n",
    "        self.ext2int_user_map, self.int2ext_user_map, self.ext2int_item_map, self.int2ext_item_map = maps\n",
    "        self.dataset = self.format_dataset(dataset)\n",
    "        self.rated_items = {\n",
    "            self.ext2int_user_map[u]: dataset[(dataset.iloc[:, 0] == u) & (dataset.iloc[:, 2] >= relevance)].iloc[:,\n",
    "                                      1].map(self.ext2int_item_map).astype(int).to_list() for u in\n",
    "            self.ext2int_user_map}\n",
    "        n_users = len(self.ext2int_user_map)\n",
    "        n_items = len(self.ext2int_item_map)\n",
    "        self.n_interactions = len(dataset)\n",
    "        self.delta_ratings = dataset.iloc[:, 2].max() - dataset.iloc[:, 2].min()\n",
    "        self.p = np.random.normal(size=(n_users, n_factors), scale=1./n_factors, loc=0)\n",
    "        self.q = np.random.normal(size=(n_items, n_factors), scale=1./n_factors, loc=0)\n",
    "\n",
    "        self.b_u = np.zeros(n_users)\n",
    "        self.b_i = np.zeros(n_items)\n",
    "        self.b = np.mean(dataset['rating'])\n",
    "\n",
    "        self.i_avg = None\n",
    "        self.u_avg = None\n",
    "        if i_avg is not None and u_avg is not None:\n",
    "            self.i_avg = i_avg.to_numpy()\n",
    "            self.u_avg = u_avg.to_numpy()\n",
    "\n",
    "    def format_dataset(self, df):\n",
    "        dataset = {}\n",
    "        dataset['userId'] = df.iloc[:, 0].map(self.ext2int_user_map).to_dict()\n",
    "        dataset['itemId'] = df.iloc[:, 1].map(self.ext2int_item_map).to_dict()\n",
    "        dataset['rating'] = df.iloc[:, 2].to_dict()\n",
    "        return dataset\n",
    "\n",
    "    def train(self, lr, beta, epochs):\n",
    "        print(\"Starting training...\")\n",
    "        for e in range(epochs):\n",
    "            print(f\"*** Epoch {e + 1}/{epochs} ***\")\n",
    "            for i in tqdm(range(self.n_interactions)):\n",
    "                p_u = self.p[self.dataset['userId'][i]]\n",
    "                q_i = self.q[self.dataset['itemId'][i]]\n",
    "                pred = self.b + self.b_u[self.dataset['userId'][i]] + self.b_i[self.dataset['itemId'][i]] + p_u.dot(q_i)\n",
    "                err = self.dataset['rating'][i] - pred\n",
    "\n",
    "                # Update biases\n",
    "                self.b_u[self.dataset['userId'][i]] += lr * (err - beta * self.b_u[self.dataset['userId'][i]])\n",
    "                self.b_i[self.dataset['itemId'][i]] += lr * (err - beta * self.b_i[self.dataset['itemId'][i]])\n",
    "\n",
    "                # Update embeddings\n",
    "                self.p[self.dataset['userId'][i]] = p_u + lr * (err * q_i - beta * p_u)\n",
    "                self.q[self.dataset['itemId'][i]] = q_i + lr * (err * p_u - beta * q_i)\n",
    "\n",
    "    def train_laplace_dp(self, lr, beta, epochs, eps, err_max=None):\n",
    "        for e in range(epochs):\n",
    "            print(f\"*** Epoch {e + 1}/{epochs} ***\")\n",
    "            for i in tqdm(range(self.n_interactions)):\n",
    "                p_u = self.p[self.dataset['userId'][i]]\n",
    "                q_i = self.q[self.dataset['itemId'][i]]\n",
    "                pred = self.b + self.b_u[self.dataset['userId'][i]] + self.b_i[self.dataset['itemId'][i]] + p_u.dot(q_i)\n",
    "\n",
    "                # We should add Laplacian noise scaled based on the sensitivity of the error\n",
    "                # Having two datasets A and B differing by the value of just one rating r_ui (namely r_ui(A) and r_ui(B)), we have that\n",
    "                # max |e(A) - e(B)| = max |(r_ui(A) - p_u * q_i) - (r_ui(B) - p_u * q_i)| = max |r_ui(A) - r_ui(B)| = delta_ratings\n",
    "                # For the composability theorem, we have to split the privacy budget by the number of epochs\n",
    "                err = self.dataset['rating'][i] - pred + np.random.laplace(scale=(epochs * self.delta_ratings / eps))\n",
    "\n",
    "                # Optionally we can constrain the error the limit the effect of the noise\n",
    "                if err_max:\n",
    "                    err = np.clip(err, -err_max, err_max)\n",
    "\n",
    "                # Update biases\n",
    "                self.b_u[self.dataset['userId'][i]] += lr * (err - beta * self.b_u[self.dataset['userId'][i]])\n",
    "                self.b_i[self.dataset['itemId'][i]] += lr * (err - beta * self.b_i[self.dataset['itemId'][i]])\n",
    "\n",
    "                self.p[self.dataset['userId'][i]] = p_u + lr * (err * q_i)\n",
    "                self.q[self.dataset['itemId'][i]] = q_i + lr * (err * p_u)\n",
    "\n",
    "\n",
    "    def evaluate(self, test=None, cutoff=10, relevance=0.5):\n",
    "        print(\"Starting evaluation...\")\n",
    "        if self.i_avg is not None and self.u_avg is not None:\n",
    "            prediction = self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis, :] + \\\n",
    "                         (np.dot(self.p, self.q.T).T + self.i_avg[:, None]).T + self.u_avg[:, None]\n",
    "        else:\n",
    "            prediction = self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis, :] + np.dot(self.p, self.q.T)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        print(\"Reading test set...\")\n",
    "        relevant_items_test = {\n",
    "            self.ext2int_user_map[u]: set(test[(test.iloc[:, 0] == u) & (test.iloc[:, 2] >= relevance)].iloc[:, 1].map(\n",
    "                self.ext2int_item_map).dropna().astype(int).to_list()) for u in self.ext2int_user_map}\n",
    "        print(\"Computing metrics...\")\n",
    "        for u in self.int2ext_user_map:\n",
    "            prediction[u, self.rated_items[u]] = - np.inf\n",
    "            unordered_top_k = np.argpartition(prediction[u], -cutoff)[-cutoff:]\n",
    "            top_k = unordered_top_k[np.argsort(prediction[u][unordered_top_k])][::-1]\n",
    "            n_rel_and_rec_k = sum(i in relevant_items_test[u] for i in top_k)\n",
    "            precisions.append(n_rel_and_rec_k / cutoff)\n",
    "            try:\n",
    "                recalls.append(n_rel_and_rec_k / len(relevant_items_test[u]))\n",
    "            except ZeroDivisionError:\n",
    "                recalls.append(0)\n",
    "        precision = sum(precisions) / len(precisions)\n",
    "        recall = sum(recalls) / len(recalls)\n",
    "\n",
    "        print(f\"Precision@{cutoff}: {precision}\")\n",
    "        print(f\"Recall@{cutoff}: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and Train The Model\n",
    "Now, we are ready to initialize and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n",
      "*** Epoch 1/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 25966.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 2/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22218.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 3/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22822.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 4/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22548.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 5/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 23094.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 6/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 21860.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 7/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22629.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 8/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22374.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 9/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22753.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 10/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22833.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 11/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22759.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 12/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22823.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 13/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 21602.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 14/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 21118.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 15/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22468.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 16/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22542.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 17/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22369.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 18/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22669.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 19/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22484.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 20/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 23249.09it/s]\n"
     ]
    }
   ],
   "source": [
    "f = 100\n",
    "lr = 0.001\n",
    "lmb = 0.1\n",
    "epochs = 20\n",
    "\n",
    "mf = MF(train_set, maps, f, relevance=4)\n",
    "mf.train(lr, lmb, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate The Model\n",
    "\n",
    "The evaluation is computed on Top-K recommendation lists (default K = 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Reading test set...\n",
      "Computing metrics...\n",
      "Precision@10: 0.058524590163934534\n",
      "Recall@10: 0.03103142768420843\n"
     ]
    }
   ],
   "source": [
    "mf.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build DP schema on dataset\n",
    "\n",
    "Before feeding our recommender, we preprocess the dataset by measuring noisy versions of some global effects. In detail we measure:\n",
    "* a differentially private version of the global average\n",
    "* a differentially private version of the item averages\n",
    "* a differentially private version of the user averages\n",
    "\n",
    "Finally, we clamp the resulting ratings\n",
    "\n",
    "This preprocessing is proven to allow deriving more accurate predictions when using the MF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def privatize_global_effects(ratings, b_m, b_u, eps_global_avg, eps_item_avg, eps_user_avg, clamping):\n",
    "    min_rating = ratings['rating'].min()\n",
    "    max_rating = ratings['rating'].max()\n",
    "    delta_r = max_rating - min_rating\n",
    "\n",
    "    # Measure the noisy version\n",
    "\n",
    "    global_average_item = (ratings['rating'].sum() + np.random.laplace(scale=(delta_r / eps_global_avg))) / len(ratings)\n",
    "\n",
    "    item_sets = ratings.groupby('movieId')['rating']\n",
    "    i_avg = (item_sets.sum() + b_m * global_average_item + np.random.laplace(scale=(delta_r / eps_item_avg),\n",
    "                                                                             size=len(item_sets))) / (\n",
    "                        item_sets.count() + b_m)\n",
    "    i_avg = np.clip(i_avg, min_rating, max_rating)\n",
    "\n",
    "    merged = ratings.join(i_avg, on=['movieId'], lsuffix='_x', rsuffix='_y')\n",
    "\n",
    "    merged['rating'] = merged['rating_x'] - merged['rating_y']\n",
    "    merged = merged.drop(columns=['rating_x', 'rating_y'], axis=1)\n",
    "\n",
    "    global_average_user = (merged['rating'].sum() + np.random.laplace(scale=(delta_r / eps_global_avg))) / len(merged)\n",
    "\n",
    "    user_sets = merged.groupby('userId')['rating']\n",
    "    u_avg = (user_sets.sum() + b_u * global_average_user + np.random.laplace(scale=(delta_r / eps_user_avg))) / (\n",
    "                user_sets.count() + b_u)\n",
    "    u_avg = np.clip(u_avg, -2, 2)\n",
    "    # Values come from the implementation of this approach by Friedman et al., 2016\n",
    "\n",
    "    preprocessed_ratings = merged.join(u_avg, on=['userId'], lsuffix='_x', rsuffix='_y')\n",
    "\n",
    "    preprocessed_ratings['rating'] = preprocessed_ratings['rating_x'] - preprocessed_ratings['rating_y']\n",
    "    preprocessed_ratings = preprocessed_ratings.drop(columns=['rating_x', 'rating_y'], axis=1)\n",
    "    preprocessed_ratings['rating'] = np.clip(preprocessed_ratings['rating'], -clamping, clamping)\n",
    "\n",
    "    return preprocessed_ratings, i_avg, u_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From Friedman et. al, 2016, having a eps privacy budget, we can split it with the following proportion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eps = 5\n",
    "eps_global_avg = 0.02 * eps\n",
    "eps_item_avg = 0.14 * eps\n",
    "eps_user_avg = 0.14 * eps\n",
    "# Overall, we used 0.3 of our eps for the preprocessing\n",
    "# The remaing 0.7 of eps is used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's preprocess the data based on the protocol we analyzed so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b_m = 5\n",
    "b_u = 5\n",
    "clamping = 1\n",
    "preproc_train_set, i_avg, u_avg = privatize_global_effects(train_set, b_m, b_u, eps_global_avg, eps_item_avg,\n",
    "                                                           eps_user_avg, clamping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's explore the first perturbation strategy, applied on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Starting training...\n",
      "*** Epoch 1/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20886.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 2/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22273.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 3/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22172.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 4/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22382.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 5/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22707.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 6/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 23141.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 7/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22138.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 8/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22088.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 9/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22610.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 10/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22310.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 11/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 21919.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 12/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22070.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 13/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22494.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 14/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22218.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 15/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22769.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 16/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22172.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 17/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22272.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 18/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 23010.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 19/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22755.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 20/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 22211.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Reading test set...\n",
      "Computing metrics...\n",
      "Precision@10: 0.01262295081967212\n",
      "Recall@10: 0.005014173680719737\n"
     ]
    }
   ],
   "source": [
    "def input_perturbation(ratings, clamping, eps):\n",
    "    # Range of the received ratings is [-clamping, clamping]\n",
    "    # In the input perturbation approach, each rating is perturbed in a way that maintains differential privacy\n",
    "    delta_r = 2 * clamping  # global sensitivity\n",
    "\n",
    "    # differential privacy\n",
    "    perturbed_ratings = ratings.copy()\n",
    "    perturbed_ratings['rating'] = np.clip(ratings['rating'] + np.random.laplace(scale=(delta_r / eps), size=len(ratings)),\n",
    "                                -clamping, clamping)\n",
    "    \n",
    "    return perturbed_ratings\n",
    "\n",
    "train_set_perturbed = input_perturbation(train_set, 1, 0.7 * eps)\n",
    "mf_dp_data = MF(train_set_perturbed, maps, f, relevance=4, i_avg=i_avg, u_avg=u_avg)\n",
    "mf_dp_data.train(lr, lmb, epochs)\n",
    "mf_dp_data.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the second approach, with the differential privacy applied to the SGD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "*** Epoch 1/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20293.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 2/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20131.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 3/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19870.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 4/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19883.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 5/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19988.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 6/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20182.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 7/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20171.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 8/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19659.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 9/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 18981.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 10/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 18039.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 11/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19125.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 12/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19438.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 13/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19986.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 14/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:03<00:00, 20483.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 15/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19873.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 16/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 20033.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 17/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19916.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 18/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 20056.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 19/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19999.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 20/20 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80419/80419 [00:04<00:00, 19978.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Reading test set...\n",
      "Computing metrics...\n",
      "Precision@10: 0.013606557377049166\n",
      "Recall@10: 0.005783037926487868\n"
     ]
    }
   ],
   "source": [
    "mf_dp_sgd = MF(train_set, maps, f, relevance=4, i_avg=i_avg, u_avg=u_avg)\n",
    "mf_dp_sgd.train_laplace_dp(lr, lmb, epochs, 0.7 * eps)\n",
    "mf_dp_sgd.evaluate(test_set)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}